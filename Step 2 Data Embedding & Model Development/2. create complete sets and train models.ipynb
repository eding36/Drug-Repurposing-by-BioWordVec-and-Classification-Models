{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-09-03T14:49:01.967399Z",
     "start_time": "2024-09-03T14:49:01.562746Z"
    }
   },
   "source": [
    "###1 Extracting all the names of each positive triple from dataset\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "parent_dir = os.path.dirname(os.getcwd())\n",
    "print(parent_dir)\n",
    "\n",
    "unified_columns = [f\"{i}\" for i in range(604)]\n",
    "\n",
    "df = pd.read_csv(os.path.join(parent_dir, 'Step 1 Data Processing/ROBOKOP+DrugMechDB/ROBOKOP+DrugmechDB Data/ROBOMechDB Processed Triples.csv'))\n",
    "df = df.drop(['Unnamed: 0'],axis=1)\n",
    "positive_triples_array = []\n",
    "\n",
    "triples_drug_keys = df['drug_name']\n",
    "triples_disease_keys = df['disease_name']\n",
    "triples_protein_keys = df['protein_name']\n",
    "\n",
    "unique_triples_drug = sorted(list(set(df['drug_name'])))\n",
    "unique_triples_disease =sorted(list(set(df['disease_name'])))\n",
    "unique_triples_protein = sorted(list(set(df['protein_name'])))\n",
    "\n",
    "triples_dictionary = {}\n",
    "values = my_list = [1] * len(triples_drug_keys)\n",
    "keys = [triples_drug_keys[i] + \" \" + triples_disease_keys[i]+ \" \" + triples_protein_keys[i] for i in range(0,len(triples_drug_keys))]\n",
    "for key,value in zip(keys,values):\n",
    "    triples_dictionary[key] = value\n",
    "\n",
    "for i in range(len(triples_protein_keys)):\n",
    "    triple = [triples_drug_keys[i],triples_disease_keys[i],triples_protein_keys[i]]\n",
    "    positive_triples_array.append(triple)\n",
    "\n",
    "positive_triples_array = np.array(positive_triples_array)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/eding/PycharmProjects/U24-ROBOKOP-Project-8-21-24\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-03T14:49:05.018369Z",
     "start_time": "2024-09-03T14:49:04.919343Z"
    }
   },
   "cell_type": "code",
   "source": [
    "####2 Importing created protein, disease, and drug vector dictionaries into here\n",
    "\n",
    "def csvtodict(filename):\n",
    "    vector_dict = {}\n",
    "    df_vector_dict = pd.read_csv(filename, header = 0)\n",
    "    keys = df_vector_dict.iloc[:,0].tolist()\n",
    "    values = list(range(len(df_vector_dict)))\n",
    "    for key,value in zip(keys,values):\n",
    "        vector_dict[key] = value\n",
    "    return vector_dict\n",
    "\n",
    "protein_dict = csvtodict(os.path.join(parent_dir, 'Step 2 Data Embedding & Model Development/Vector Dictionaries/ROBOMechDB Protein Vector Dictionary.csv'))\n",
    "\n",
    "disease_dict = csvtodict(os.path.join(parent_dir, 'Step 2 Data Embedding & Model Development/Vector Dictionaries/ROBOMechDB Disease Vector Dictionary.csv'))\n",
    "\n",
    "drug_dict = csvtodict(os.path.join(parent_dir, 'Step 2 Data Embedding & Model Development/Vector Dictionaries/ROBOMechDB Drug Vector Dictionary.csv'))\n",
    "\n",
    "protein_df = pd.read_csv(os.path.join(parent_dir, 'Step 2 Data Embedding & Model Development/Vector Dictionaries/ROBOMechDB Protein Vector Dictionary.csv'),header = 0)\n",
    "disease_df = pd.read_csv(os.path.join(parent_dir, 'Step 2 Data Embedding & Model Development/Vector Dictionaries/ROBOMechDB Disease Vector Dictionary.csv'),header = 0)\n",
    "drug_df = pd.read_csv(os.path.join(parent_dir, 'Step 2 Data Embedding & Model Development/Vector Dictionaries/ROBOMechDB Drug Vector Dictionary.csv'), header = 0)"
   ],
   "id": "68946a5983a6dabc",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-03T14:49:05.745237Z",
     "start_time": "2024-09-03T14:49:05.743035Z"
    }
   },
   "cell_type": "code",
   "source": "print(positive_triples_array)",
   "id": "c7829ea20780ca3f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['imatinib' 'chronic myelogenous leukemia, bcr-abl1 positive'\n",
      "  'abl proto-oncogene 1, non-receptor tyrosine kinase']\n",
      " ['imatinib' 'systemic mastocytosis'\n",
      "  'kit proto-oncogene, receptor tyrosine kinase']\n",
      " ['imatinib' 'systemic mastocytosis'\n",
      "  'platelet derived growth factor receptor alpha']\n",
      " ...\n",
      " ['theophylline'\n",
      "  'asthma-chronic obstructive pulmonary disease overlap syndrome'\n",
      "  'phosphodiesterase 4a']\n",
      " ['theophylline'\n",
      "  'asthma-chronic obstructive pulmonary disease overlap syndrome'\n",
      "  'phosphodiesterase 4d']\n",
      " ['theophylline'\n",
      "  'asthma-chronic obstructive pulmonary disease overlap syndrome'\n",
      "  'phosphodiesterase 4b']]\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-03T14:49:06.209532Z",
     "start_time": "2024-09-03T14:49:06.207720Z"
    }
   },
   "cell_type": "code",
   "source": "print(len(positive_triples_array))",
   "id": "922f36ad04d1e3c4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9478\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-03T14:49:07.209485Z",
     "start_time": "2024-09-03T14:49:07.207680Z"
    }
   },
   "cell_type": "code",
   "source": "print(positive_triples_array[1])",
   "id": "5fe7f847a83dc00b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['imatinib' 'systemic mastocytosis'\n",
      " 'kit proto-oncogene, receptor tyrosine kinase']\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-03T14:49:12.204612Z",
     "start_time": "2024-09-03T14:49:10.082947Z"
    }
   },
   "cell_type": "code",
   "source": [
    "###3 Building a 600 column vector array of all positive triples\n",
    "\n",
    "positive_triples_vector_array = []\n",
    "import itertools\n",
    "\n",
    "for i in range(0,len(positive_triples_array)):\n",
    "    try:\n",
    "        drug_vector = drug_df.iloc[drug_dict[positive_triples_array[i,0]],1:201].tolist()\n",
    "        disease_vector = disease_df.iloc[disease_dict[positive_triples_array[i,1]],1:201].tolist()\n",
    "        protein_vector = protein_df.iloc[protein_dict[positive_triples_array[i,2]],1:201].tolist()\n",
    "        \n",
    "        drug_name = positive_triples_array[i,0]\n",
    "       # drug_id = df.iloc[i,1]\n",
    "        \n",
    "        disease_name =positive_triples_array[i,1]\n",
    "       # disease_id = df.iloc[i,3]\n",
    "        protein_name = positive_triples_array[i,2]\n",
    "       # protein_id = df.iloc[i,5]\n",
    "        \n",
    "        row = [[drug_name],[disease_name],[protein_name],drug_vector,disease_vector,protein_vector,[1]]\n",
    "        merged = list(itertools.chain(*row))\n",
    "        positive_triples_vector_array.append(merged)\n",
    "    except KeyError:\n",
    "        continue\n",
    "\n",
    "positive_triples_dataframe = pd.DataFrame(positive_triples_vector_array)\n",
    "\n",
    "print(positive_triples_dataframe)"
   ],
   "id": "d674b2638abb6698",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               0                                                  1    \\\n",
      "0         imatinib    chronic myelogenous leukemia, bcr-abl1 positive   \n",
      "1         imatinib                              systemic mastocytosis   \n",
      "2         imatinib                              systemic mastocytosis   \n",
      "3      paracetamol                                               pain   \n",
      "4      paracetamol                                               pain   \n",
      "...            ...                                                ...   \n",
      "9177   pralidoxime          poisoning of animals with phosphate salts   \n",
      "9178  theophylline  asthma-chronic obstructive pulmonary disease o...   \n",
      "9179  theophylline  asthma-chronic obstructive pulmonary disease o...   \n",
      "9180  theophylline  asthma-chronic obstructive pulmonary disease o...   \n",
      "9181  theophylline  asthma-chronic obstructive pulmonary disease o...   \n",
      "\n",
      "                                                    2        3         4    \\\n",
      "0     abl proto-oncogene 1, non-receptor tyrosine ki...  0.78078  0.165620   \n",
      "1          kit proto-oncogene, receptor tyrosine kinase  0.78078  0.165620   \n",
      "2         platelet derived growth factor receptor alpha  0.78078  0.165620   \n",
      "3                 prostaglandin-endoperoxide synthase 1  0.65269  0.504390   \n",
      "4                 prostaglandin-endoperoxide synthase 2  0.65269  0.504390   \n",
      "...                                                 ...      ...       ...   \n",
      "9177                              butyrylcholinesterase  0.10069  0.477550   \n",
      "9178                               phosphodiesterase 4c  0.55648  0.051347   \n",
      "9179                               phosphodiesterase 4a  0.55648  0.051347   \n",
      "9180                               phosphodiesterase 4d  0.55648  0.051347   \n",
      "9181                               phosphodiesterase 4b  0.55648  0.051347   \n",
      "\n",
      "          5         6         7        8         9    ...       594       595  \\\n",
      "0    -0.48965  0.348400 -0.053361  0.66634 -0.084676  ... -2.024610 -1.207784   \n",
      "1    -0.48965  0.348400 -0.053361  0.66634 -0.084676  ... -1.481937 -0.622788   \n",
      "2    -0.48965  0.348400 -0.053361  0.66634 -0.084676  ... -2.278220  0.346472   \n",
      "3     0.28968 -0.096725 -0.395670  0.45474  0.351790  ... -1.476880 -1.330130   \n",
      "4     0.28968 -0.096725 -0.395670  0.45474  0.351790  ... -1.472300 -1.040040   \n",
      "...       ...       ...       ...      ...       ...  ...       ...       ...   \n",
      "9177 -0.70720  0.581080 -0.988220 -0.23643  0.391710  ...  0.041344  0.296120   \n",
      "9178 -0.55280  0.536750 -0.160190 -0.15023  0.383770  ... -0.187720 -0.794843   \n",
      "9179 -0.55280  0.536750 -0.160190 -0.15023  0.383770  ... -0.829710 -1.168083   \n",
      "9180 -0.55280  0.536750 -0.160190 -0.15023  0.383770  ...  0.022360 -1.080283   \n",
      "9181 -0.55280  0.536750 -0.160190 -0.15023  0.383770  ... -0.254170 -0.703753   \n",
      "\n",
      "           596       597       598       599       600       601       602  \\\n",
      "0     1.840024  1.325786 -0.661386  0.470791  0.519335 -0.474252 -2.249810   \n",
      "1     1.924234  0.540982 -0.043763  1.373380 -0.083966 -0.669512 -2.078353   \n",
      "2     0.755227 -0.049974  0.752974  0.500171  0.939089 -0.921980 -0.980339   \n",
      "3     0.210785  1.024020  0.237890 -0.064981 -0.048800 -0.862920 -0.752060   \n",
      "4     0.554085  1.014210  0.172930 -0.076911 -0.331020 -0.709189 -0.785900   \n",
      "...        ...       ...       ...       ...       ...       ...       ...   \n",
      "9177  0.372410  0.359610  0.121600  0.845040 -0.402580  0.299880 -0.149820   \n",
      "9178  1.654340  0.303569  0.221640  0.484190  0.424580 -0.081170 -1.895320   \n",
      "9179  1.386110  0.376239  0.573060  0.104930  0.513290 -0.413420 -1.034500   \n",
      "9180  1.304400  0.837729  0.619910 -0.254550  0.138737 -0.538350 -1.574070   \n",
      "9181  1.084290  0.276479  0.446616  0.346650  0.618170 -0.604680 -1.746390   \n",
      "\n",
      "      603  \n",
      "0       1  \n",
      "1       1  \n",
      "2       1  \n",
      "3       1  \n",
      "4       1  \n",
      "...   ...  \n",
      "9177    1  \n",
      "9178    1  \n",
      "9179    1  \n",
      "9180    1  \n",
      "9181    1  \n",
      "\n",
      "[9182 rows x 604 columns]\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-14T18:23:17.837977Z",
     "start_time": "2024-08-14T18:12:28.789571Z"
    }
   },
   "cell_type": "code",
   "source": [
    "###4 Use random combinations of drug, disease, protein present in positive triples to form negatives. Create negative vector array\n",
    "import random\n",
    "\n",
    "negative_triples_array = []\n",
    "negative_triples_vector_array = []\n",
    "temp = set()\n",
    "i=0\n",
    "random.seed(42)\n",
    "\n",
    "while i < 1.05*len(positive_triples_array):  ###create negative triples\n",
    "    drug = random.sample(unique_triples_drug,k=1)[0]\n",
    "    protein = random.sample(unique_triples_protein, k=1)[0]\n",
    "    disease = random.sample(unique_triples_disease,k=1)[0]\n",
    "    if (drug + \" \" + disease + \" \" + protein) in triples_dictionary or (drug + \" \" + disease + \" \" + protein) in temp:\n",
    "        continue\n",
    "    temp.add(drug + \" \" + disease + \" \" + protein)\n",
    "    negative_triples_array.append([drug, disease, protein])\n",
    "    i+= 1 \n",
    "\n",
    "negative_triples_array = np.array(negative_triples_array)\n",
    "    \n",
    "for i in range(0,len(negative_triples_array)):\n",
    "    try:\n",
    "        drug_vector = drug_df.iloc[drug_dict[negative_triples_array[i,0]],1:201].tolist()\n",
    "        disease_vector = disease_df.iloc[disease_dict[negative_triples_array[i,1]],1:201].tolist()\n",
    "        protein_vector = protein_df.iloc[protein_dict[negative_triples_array[i,2]],1:201].tolist()\n",
    "        \n",
    "        drug_name = negative_triples_array[i,0]\n",
    "        disease_name = negative_triples_array[i,1]\n",
    "        protein_name = negative_triples_array[i,2]\n",
    "        \n",
    "        row = [[drug_name],[disease_name],[protein_name],drug_vector,disease_vector,protein_vector,[0]]\n",
    "        merged = list(itertools.chain(*row))\n",
    "        negative_triples_vector_array.append(merged)\n",
    "\n",
    "    except KeyError:\n",
    "        continue\n",
    "\n",
    "negative_triples_dataframe = pd.DataFrame(negative_triples_vector_array)\n",
    "df1 = pd.concat([positive_triples_dataframe,negative_triples_dataframe],axis=0,ignore_index=True)\n",
    "\n",
    "df1.columns = unified_columns\n",
    "\n",
    "#df1.to_csv(os.path.join(os.getcwd(),'Complete Sets/Complete Set 1.csv'))\n",
    "####--------------------####\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import auc, roc_curve\n",
    "\n",
    "shuffled_df = df1.sample(frac=1.0, random_state=42) \n",
    "data = shuffled_df.iloc[:, 3:-1].values\n",
    "labels = shuffled_df.iloc[:, -1].values\n",
    "print(shuffled_df)\n",
    "num_folds = 5\n",
    "skf = StratifiedKFold(n_splits=num_folds, shuffle=True, random_state=42)\n",
    "\n",
    "all_predictions = []\n",
    "all_test_sets = []\n",
    "\n",
    "    # Perform 5-fold cross-validation to test the accuracy of the model\n",
    "fold = 1\n",
    "auc_values = []\n",
    "\n",
    "tf.keras.utils.set_random_seed(42)\n",
    "\n",
    "for train_index, test_index in skf.split(data, labels):\n",
    "    print(f\"Fold: {fold}\")\n",
    "    \n",
    "    # Split the data into training and test sets\n",
    "    X_train, X_test = data[train_index], data[test_index]\n",
    "    y_train, y_test = labels[train_index], labels[test_index]\n",
    "    tf_train_set = tf.data.Dataset.from_tensor_slices((X_train,y_train))\n",
    "    tf_test_set = tf.data.Dataset.from_tensor_slices((X_test,y_test))\n",
    "        \n",
    "    BATCH_SIZE = 32\n",
    "    STEPS_PER_EPOCH = len(X_train)//BATCH_SIZE\n",
    "    tf_train_set = tf_train_set.repeat().batch(BATCH_SIZE)\n",
    "    tf_test_set = tf_test_set.batch(BATCH_SIZE)\n",
    "\n",
    "    # Define the learning rate schedule\n",
    "    lr_schedule = tf.keras.optimizers.schedules.InverseTimeDecay(\n",
    "        0.001,\n",
    "        decay_steps=STEPS_PER_EPOCH * 1000,\n",
    "        decay_rate=1,\n",
    "        staircase=False\n",
    "    )\n",
    "        \n",
    "    def get_optimizer():\n",
    "        return tf.keras.optimizers.Adam(lr_schedule)\n",
    "\n",
    "    optimizer = get_optimizer()\n",
    "        \n",
    "    model = tf.keras.models.Sequential([\n",
    "        tf.keras.Input(shape=(600,)),\n",
    "        tf.keras.layers.Dense(1024, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001)), \n",
    "        tf.keras.layers.Dropout(0.8),\n",
    "        tf.keras.layers.Dense(512, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001)),\n",
    "        tf.keras.layers.Dropout(0.8),\n",
    "        tf.keras.layers.Dense(256, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001)),\n",
    "        tf.keras.layers.Dropout(0.8),\n",
    "        tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "\n",
    "    model.compile(optimizer=optimizer,\n",
    "                loss='binary_crossentropy',\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "    # Implement early stopping\n",
    "    early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "        \n",
    "    # Train the model\n",
    "    model.fit(tf_train_set,\n",
    "                    steps_per_epoch=STEPS_PER_EPOCH, \n",
    "                    epochs=40,\n",
    "                    validation_data = tf_test_set, \n",
    "                    callbacks=[early_stopping], \n",
    "                    verbose=0)\n",
    "        \n",
    "    predictions = model.predict(X_test)\n",
    "    fpr,tpr, thresholds = roc_curve(y_test,predictions)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    auc_values.append(roc_auc)\n",
    "    print(\"AUROC for test data:\", roc_auc)\n",
    "    all_predictions.append(predictions)\n",
    "    all_test_sets.append(y_test)\n",
    "        \n",
    "    ####Before each iteration ends, save each model###\n",
    "        \n",
    "    save_model= tf.keras.models.save_model(model, os.path.join(os.getcwd(), f'Classification Models/Complete Set 1 Models/ROBOMechDB Complete Set 1 Model Fold {fold}.keras'))\n",
    "\n",
    "    fold += 1\n",
    "        \n",
    "\n",
    "fcv_mean_auc = np.mean(auc_values)\n",
    "fcv_std_auc = np.std(auc_values)\n",
    "fcv_min_auc = np.min(auc_values)\n",
    "fcv_max_auc = np.max(auc_values)\n"
   ],
   "id": "3e8ba1ff0e6aefb6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                           0                                      1  \\\n",
      "4097            theophylline                             bronchitis   \n",
      "8528              nintedanib              interstitial lung disease   \n",
      "7621    ethynodiol diacetate              polycystic ovary syndrome   \n",
      "4754              salbutamol  chronic obstructive pulmonary disease   \n",
      "15905          prednicarbate                 blepharoconjunctivitis   \n",
      "...                      ...                                    ...   \n",
      "11284          isocarboxazid                       male infertility   \n",
      "11964  loteprednol etabonate     thyroid gland follicular carcinoma   \n",
      "5390               sunitinib                   renal cell carcinoma   \n",
      "860              metaraminol                   hypotensive disorder   \n",
      "15795              icatibant               meningococcal meningitis   \n",
      "\n",
      "                                                    2         3         4  \\\n",
      "4097                            adenosine a1 receptor  0.556480  0.051347   \n",
      "8528              fibroblast growth factor receptor 3  0.616580  0.393360   \n",
      "7621                              estrogen receptor 1  1.674080 -0.622570   \n",
      "4754                              adrenoceptor beta 2  0.717000  0.471190   \n",
      "15905                            nuak family kinase 2 -0.100140  0.006144   \n",
      "...                                               ...       ...       ...   \n",
      "11284        homeodomain interacting protein kinase 4  0.438320 -0.180560   \n",
      "11964                    catechol-o-methyltransferase  0.730500 -0.347620   \n",
      "5390                      serine/threonine kinase 17a  0.643790  0.202160   \n",
      "860                             adrenoceptor alpha 1a -0.189900  0.300250   \n",
      "15795  calcium voltage-gated channel subunit alpha1 b -0.014498  0.018683   \n",
      "\n",
      "              5         6         7         8         9  ...       594  \\\n",
      "4097  -0.552800  0.536750 -0.160190 -0.150230  0.383770  ... -1.061320   \n",
      "8528   0.219060 -0.005049 -0.285270  0.650520 -0.117420  ... -1.595830   \n",
      "7621  -0.060370  0.182923 -0.808545 -2.013050  0.466260  ... -1.014550   \n",
      "4754  -0.059094  0.618560 -0.094258 -0.065898  0.060800  ... -1.428290   \n",
      "15905 -0.153510  0.326740 -0.122170 -0.171190 -0.048322  ... -0.927332   \n",
      "...         ...       ...       ...       ...       ...  ...       ...   \n",
      "11284 -0.587940 -0.013347 -0.135800 -0.533250  0.711020  ... -1.256030   \n",
      "11964 -0.021131  1.357930 -0.005000  0.383190 -0.708200  ... -0.497720   \n",
      "5390  -0.217640  0.267660 -0.418300  0.482550 -0.202170  ... -0.407972   \n",
      "860   -0.647380 -0.160910 -0.361150 -0.182860  0.115960  ... -0.086920   \n",
      "15795 -0.209540  0.863850 -0.516130  0.048650  0.124280  ... -0.215699   \n",
      "\n",
      "            595       596       597       598       599       600       601  \\\n",
      "4097  -0.967350  0.591424  0.164056  1.459400  0.104000 -0.349590  0.283850   \n",
      "8528  -0.177599  0.834116  0.449698  0.633586  0.779050  0.920278 -0.361060   \n",
      "7621  -0.413290  0.592599  0.186903 -0.266550  0.006380  0.554703 -0.367745   \n",
      "4754  -0.280070  1.066838  0.075221 -0.954710  0.056960  0.565281 -0.252924   \n",
      "15905 -0.186460  1.163657  0.399658 -1.302260 -0.642790  0.302590 -0.026079   \n",
      "...         ...       ...       ...       ...       ...       ...       ...   \n",
      "11284  0.331732  1.581411  0.071171 -0.326576 -0.090700  1.137530 -1.216132   \n",
      "11964 -0.751180 -0.259240  0.243970 -0.009285  0.599510 -0.055454 -0.346470   \n",
      "5390  -1.500458  1.441580  0.386920 -0.682170  0.267374 -0.067930 -1.361621   \n",
      "860   -1.532920  0.783298  0.239791  0.117088  0.008277  0.190644 -0.941985   \n",
      "15795  0.421690  2.318850 -0.745840 -0.172370 -0.651797  1.942074 -2.733258   \n",
      "\n",
      "            602  603  \n",
      "4097  -0.739319    1  \n",
      "8528  -0.929209    1  \n",
      "7621   0.094871    1  \n",
      "4754   0.549630    1  \n",
      "15905  0.113456    0  \n",
      "...         ...  ...  \n",
      "11284 -1.418699    0  \n",
      "11964 -0.168710    0  \n",
      "5390  -2.630080    1  \n",
      "860   -0.572960    1  \n",
      "15795 -3.622836    0  \n",
      "\n",
      "[18301 rows x 604 columns]\n",
      "Fold: 1\n",
      "\u001B[1m115/115\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 1ms/step\n",
      "AUROC for test data: 0.9693928530498811\n",
      "Fold: 2\n",
      "\u001B[1m115/115\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 1ms/step\n",
      "AUROC for test data: 0.9664037008514264\n",
      "Fold: 3\n",
      "\u001B[1m115/115\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 1ms/step\n",
      "AUROC for test data: 0.97236047806062\n",
      "Fold: 4\n",
      "\u001B[1m115/115\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 1ms/step\n",
      "AUROC for test data: 0.9661126877651647\n",
      "Fold: 5\n",
      "\u001B[1m115/115\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 1ms/step\n",
      "AUROC for test data: 0.9688248014849216\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-14T03:22:51.101667Z",
     "start_time": "2024-08-14T03:22:51.101615Z"
    }
   },
   "cell_type": "code",
   "source": [
    "####5. Generate tpr, tnr, conduct train:test ratio experiment\n",
    "\n",
    "all_predictions_array = np.round(np.concatenate(all_predictions))\n",
    "all_test_sets_array = np.concatenate(all_test_sets)\n",
    "\n",
    "tp_count = 0\n",
    "fp_count = 0\n",
    "fn_count = 0\n",
    "tn_count = 0\n",
    "\n",
    "for i in range(0,len(all_test_sets_array)):\n",
    "    if(all_test_sets_array[i]==1 and int(all_predictions_array[i]) ==1):\n",
    "        tp_count = tp_count+1\n",
    "\n",
    "for i in range(0,len(all_test_sets_array)):\n",
    "     if(all_test_sets_array[i]==0 and int(all_predictions_array[i])==1):\n",
    "        fp_count =fp_count+1\n",
    "\n",
    "for i in range(0,len(all_test_sets_array)):\n",
    "    if(all_test_sets_array[i]==1 and int(all_predictions_array[i])==0):\n",
    "        fn_count = fn_count+1\n",
    "\n",
    "for i in range(0,len(all_test_sets_array)):\n",
    "    if(all_test_sets_array[i]==0 and int(all_predictions_array[i])==0):\n",
    "        tn_count = tn_count+1\n",
    "\n",
    "tpr = tp_count/(tp_count+fn_count)\n",
    "tnr = tn_count/(tn_count+fp_count)\n",
    "\n",
    "print('tpr:' , tpr)\n",
    "print('tnr:' , tnr)\n",
    "print(fcv_min_auc,fcv_max_auc,fcv_std_auc,fcv_mean_auc)\n",
    "\n",
    "#####Varying Train:test ratio###\n",
    "\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.metrics import precision_recall_curve, recall_score\n",
    "import numpy as np\n",
    "import itertools\n",
    "\n",
    "stat_array = []\n",
    "train_var = [0.8,0.7,0.6,0.5,0.4,0.3,0.2,0.005]\n",
    "random_states = [42,21,32,10,12]\n",
    "\n",
    "for i in train_var:\n",
    "    print('training:testing:', i*100,(1-i)*100)\n",
    "    auc_values = []\n",
    "    precision_values = []\n",
    "    recall_values = []\n",
    "    for j in range(5):\n",
    "        tf.keras.utils.set_random_seed(j)\n",
    "        shuffled_df = df1.sample(frac=1.0, random_state=random_states[j])\n",
    "        data = np.array(shuffled_df.iloc[:,3:-1])\n",
    "        labels = np.array(shuffled_df.iloc[:,-1])\n",
    "        shuffled_df = []\n",
    "\n",
    "        train_ratio = i\n",
    "        train_size = int(train_ratio * len(data))\n",
    "        train_data = data[:train_size]\n",
    "        test_data = data[train_size:]\n",
    "        train_labels = labels[:train_size]\n",
    "        test_labels = labels[train_size:]\n",
    "        tf_train_set = tf.data.Dataset.from_tensor_slices((train_data,train_labels))\n",
    "        tf_test_set = tf.data.Dataset.from_tensor_slices((test_data,test_labels))\n",
    "        \n",
    "        BATCH_SIZE = 32\n",
    "        STEPS_PER_EPOCH = len(train_data)//BATCH_SIZE\n",
    "        tf_train_set = tf_train_set.repeat().batch(BATCH_SIZE)\n",
    "        tf_test_set = tf_test_set.batch(BATCH_SIZE)\n",
    "\n",
    "    # Define the learning rate schedule\n",
    "        lr_schedule = tf.keras.optimizers.schedules.InverseTimeDecay(\n",
    "            0.001,\n",
    "            decay_steps=STEPS_PER_EPOCH * 1000,\n",
    "            decay_rate=1,\n",
    "            staircase=False\n",
    "        )\n",
    "        \n",
    "        def get_optimizer():\n",
    "            return tf.keras.optimizers.Adam(lr_schedule)\n",
    "\n",
    "        optimizer = get_optimizer()\n",
    "        \n",
    "        model = tf.keras.models.Sequential([\n",
    "            tf.keras.Input(shape=(600,)),\n",
    "            tf.keras.layers.Dense(1024, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001)), \n",
    "            tf.keras.layers.Dropout(0.8),\n",
    "            tf.keras.layers.Dense(512, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001)),\n",
    "            tf.keras.layers.Dropout(0.8),\n",
    "            tf.keras.layers.Dense(256, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001)),\n",
    "            tf.keras.layers.Dropout(0.8),\n",
    "            tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "        ])\n",
    "\n",
    "        model.compile(optimizer=optimizer,\n",
    "                loss='binary_crossentropy',\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "        # Implement early stopping\n",
    "        early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "        \n",
    "        # Train the model\n",
    "        model.fit(tf_train_set,\n",
    "                    steps_per_epoch=STEPS_PER_EPOCH, \n",
    "                    epochs=40,\n",
    "                    validation_data = tf_test_set, \n",
    "                    callbacks=[early_stopping], \n",
    "                    verbose=0)\n",
    "        \n",
    "        predictions = model.predict(test_data)\n",
    "        test_predictions_rounded = np.round(predictions)\n",
    "        fpr, tpr, thresholds = roc_curve(test_labels, predictions)\n",
    "        precision,recall,thresholds = precision_recall_curve(test_labels, predictions)\n",
    "        auc_precision_recall = auc(recall, precision)\n",
    "        recall_score_pred = recall_score(test_labels, test_predictions_rounded)\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        auc_values.append(roc_auc)\n",
    "        precision_values.append(auc_precision_recall)\n",
    "        recall_values.append(recall_score_pred)\n",
    "    \n",
    "    print('auc avg:', np.mean(auc_values), 'auc stddev:', np.std(auc_values))\n",
    "    print('auprc avg:', np.mean(precision_values), 'auprc stddev:', np.std(precision_values))\n",
    "    print('recall avg:', np.mean(recall_values), 'recall stddev:', np.std(recall_values))\n",
    "    stat_array.append([np.mean(auc_values),np.mean(precision_values),np.mean(recall_values)])\n",
    "    \n",
    "    #### Exporting AUROC, AUPRC, and Recall values for each train test split as csv\n",
    "import csv\n",
    "statfile = \"stat.csv\"\n",
    "\n",
    "with open(statfile, 'w', newline='') as csvfile:\n",
    "    csvwriter = csv.writer(csvfile)\n",
    "    \n",
    "    # Writing the data row by row\n",
    "    for row in stat_array:\n",
    "        csvwriter.writerow(row)\n"
   ],
   "id": "c6ac0af01be9b786",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import csv\n",
    "statfile = \"stat.csv\"\n",
    "\n",
    "with open(statfile, 'w', newline='') as csvfile:\n",
    "    csvwriter = csv.writer(csvfile)\n",
    "    \n",
    "    # Writing the data row by row\n",
    "    for row in stat_array:\n",
    "        csvwriter.writerow(row)"
   ],
   "id": "e6b4b9249ec2bcb9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "6ca9f09f2548d8f9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "81e8de6b92359f79",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "edf438ca4d21a1c2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-14T03:37:24.422718Z",
     "start_time": "2024-08-14T03:23:11.832556Z"
    }
   },
   "cell_type": "code",
   "source": [
    "####5. Complete Set 2 Development and Model Training\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import auc, roc_curve\n",
    "\n",
    "unique_protein_vectors = np.array(protein_df.iloc[:,1:201])\n",
    "unique_disease_vectors = np.array(disease_df.iloc[:,1:201])\n",
    "unique_drug_vectors = np.array(drug_df.iloc[:,1:201])\n",
    "\n",
    "unique_protein_dict_names = np.array(protein_df.iloc[:,0])\n",
    "unique_disease_dict_names = np.array(disease_df.iloc[:,0])\n",
    "unique_drug_dict_names = np.array(drug_df.iloc[:,0])\n",
    "\n",
    "\n",
    "def compute_threshold(vector_dict,n=0.5):\n",
    "    distances = []\n",
    "    for i in range((len(vector_dict)-1)):\n",
    "        smallest_distance = 10000000\n",
    "        for j in range(i+1, len(vector_dict)):\n",
    "            distance = np.linalg.norm(vector_dict[i]-vector_dict[j])\n",
    "            if distance < smallest_distance:\n",
    "                smallest_distance = distance\n",
    "        distances.append(smallest_distance)\n",
    "\n",
    "    distances = np.array(distances)\n",
    "    print(len(distances))\n",
    "    print('min', np.min(distances))\n",
    "    mean_distances = np.sum(distances)/len(distances)\n",
    "    stdev_distance = np.std(distances)\n",
    "    print('mean',mean_distances)\n",
    "    print('stdev',stdev_distance)\n",
    "\n",
    "    applicability_domain = mean_distances+(n*stdev_distance)\n",
    "\n",
    "    return applicability_domain\n",
    "\n",
    "drug_threshold = compute_threshold(unique_drug_vectors)\n",
    "disease_threshold = compute_threshold(unique_disease_vectors)\n",
    "protein_threshold = compute_threshold(unique_protein_vectors)\n",
    "\n",
    "print(drug_threshold)\n",
    "print(disease_threshold)\n",
    "print(protein_threshold)\n",
    "\n",
    "random_rows = positive_triples_dataframe.sample(int(len(positive_triples_dataframe)/4), random_state = 42)\n",
    "\n",
    "#we are now going to split these rows into 3 equal chunks. \n",
    "#the triples in the first chunk will have its protein parameter randomized, \n",
    "#the triples in the second chunk will have its disease parameter randomized, and so on\n",
    "\n",
    "split_rows = np.array_split(random_rows, 3)\n",
    "\n",
    "# Each part is now a separate DataFrame\n",
    "drug_disease_x, drug_x_protein, x_disease_protein = split_rows[0], split_rows[1], split_rows[2]\n",
    "\n",
    "drug_disease_x_negative_triples = []\n",
    "drug_x_protein_negative_triples = []\n",
    "x_disease_protein_negative_triples = []\n",
    "temp = set()\n",
    "\n",
    "for i in range(int(len(drug_disease_x)/1.1)):\n",
    "    reference = np.array(drug_disease_x.iloc[i, 403:603]) ###not including 401, including 601\n",
    "    reference = [float(item) for item in reference]\n",
    "    counter=0\n",
    "    for j in range(len(unique_protein_vectors)):\n",
    "        if (np.linalg.norm(reference-unique_protein_vectors[j]) < protein_threshold):\n",
    "            triple_name = [drug_disease_x.iloc[i,0], drug_disease_x.iloc[i,1], unique_protein_dict_names[j]]\n",
    "            new_combo = drug_disease_x.iloc[i,0] + \" \" + drug_disease_x.iloc[i,1] + \" \" + unique_protein_dict_names[j]\n",
    "            if (new_combo in triples_dictionary or new_combo in temp):\n",
    "                continue\n",
    "            temp.add(new_combo)\n",
    "            triple_vector = list(itertools.chain(*[np.array(drug_disease_x.iloc[i,3:403]).tolist(), unique_protein_vectors[j].tolist()]))\n",
    "            row = list(itertools.chain(*[triple_name,triple_vector,[0]]))\n",
    "            drug_disease_x_negative_triples.append(row)\n",
    "            counter+=1\n",
    "            if counter >=6:\n",
    "                break\n",
    "\n",
    "for i in range(len(drug_x_protein)):\n",
    "    reference = np.array(drug_x_protein.iloc[i, 203:403])\n",
    "    reference = [float(item) for item in reference]\n",
    "    counter=0\n",
    "    for j in range(len(unique_disease_vectors)):\n",
    "        if (np.linalg.norm(reference-unique_disease_vectors[j]) < disease_threshold):\n",
    "            triple_name = [drug_x_protein.iloc[i,0], unique_disease_dict_names[j],drug_x_protein.iloc[i,2]]\n",
    "            new_combo = drug_x_protein.iloc[i,0] + \" \" + unique_disease_dict_names[j] + \" \" + drug_x_protein.iloc[i,2]\n",
    "            if (new_combo in triples_dictionary or new_combo in temp):\n",
    "                continue\n",
    "            temp.add(new_combo)\n",
    "            triple_vector = list(itertools.chain(*[np.array(drug_x_protein.iloc[i,3:203]).tolist(), unique_disease_vectors[j].tolist(),np.array(drug_x_protein.iloc[i,403:603]).tolist()]))\n",
    "            row = list(itertools.chain(*[triple_name,triple_vector,[0]]))\n",
    "            drug_x_protein_negative_triples.append(row)\n",
    "            counter +=1\n",
    "            if counter >=6:\n",
    "                break\n",
    "\n",
    "for i in range(len(x_disease_protein)):\n",
    "    reference = np.array(x_disease_protein.iloc[i, 3:203])\n",
    "    reference = [float(item) for item in reference]\n",
    "    counter=0    \n",
    "    for j in range(len(unique_drug_vectors)):\n",
    "        if (np.linalg.norm(reference-unique_drug_vectors[j]) < drug_threshold):\n",
    "            triple_name = [unique_drug_dict_names[j],x_disease_protein.iloc[i,1], x_disease_protein.iloc[i,2]]\n",
    "            new_combo = unique_drug_dict_names[j] + \" \" + x_disease_protein.iloc[i,1] + \" \" + x_disease_protein.iloc[i,2]\n",
    "            if (new_combo in triples_dictionary or new_combo in temp):\n",
    "                continue\n",
    "            temp.add(new_combo)\n",
    "            triple_vector = list(itertools.chain(*[unique_drug_vectors[j].tolist(),np.array(x_disease_protein.iloc[i,203:603]).tolist()]))\n",
    "            row = list(itertools.chain(*[triple_name,triple_vector,[0]]))\n",
    "            x_disease_protein_negative_triples.append(row)\n",
    "            counter += 1\n",
    "            if counter >=5:\n",
    "                break\n",
    "            \n",
    "negative_df = pd.concat([pd.DataFrame(drug_disease_x_negative_triples),pd.DataFrame(drug_x_protein_negative_triples),pd.DataFrame(x_disease_protein_negative_triples)],axis=0,ignore_index=True)\n",
    "df_2 = pd.concat([positive_triples_dataframe,negative_df],axis=0)\n",
    "df_2.columns = unified_columns\n",
    "df_2.to_csv(os.path.join(os.getcwd(),'Complete Sets/Complete Set 2.csv'), index=False)\n",
    "\n",
    "shuffled_df = df_2.sample(frac=1.0, random_state=i)\n",
    "\n",
    "data = shuffled_df.iloc[:,3:-1].values\n",
    "labels = shuffled_df.iloc[:,-1].values\n",
    "\n",
    "num_folds = 5\n",
    "skf = StratifiedKFold(n_splits=num_folds, shuffle=True, random_state=i)\n",
    "\n",
    "all_predictions = []\n",
    "all_test_sets = []\n",
    "\n",
    "# Perform 5-fold cross-validation to test the accuracy of the model######\n",
    "fold = 1\n",
    "auc_values = []\n",
    "\n",
    "keras.utils.set_random_seed(i)\n",
    "#tf.config.experimental.enable_op_determinism()\n",
    "\n",
    "shuffled_df = df_2.sample(frac=1.0, random_state=i)\n",
    "\n",
    "data = shuffled_df.iloc[:,3:-1].values\n",
    "labels = shuffled_df.iloc[:,-1].values\n",
    "\n",
    "num_folds = 5\n",
    "skf = StratifiedKFold(n_splits=num_folds, shuffle=True, random_state=i)\n",
    "\n",
    "all_predictions = []\n",
    "all_test_sets = []\n",
    "\n",
    "# Perform 5-fold cross-validation to test the accuracy of the model######\n",
    "fold = 1\n",
    "auc_values = []\n",
    "\n",
    "keras.utils.set_random_seed(i)\n",
    "    #tf.config.experimental.enable_op_determinism()\n",
    "\n",
    "for train_index, test_index in skf.split(data, labels):\n",
    "    print(f\"Fold: {fold}\")\n",
    "    print(train_index)\n",
    "    \n",
    "    # Split the data into training and test sets\n",
    "    X_train, X_test = data[train_index], data[test_index]\n",
    "    y_train, y_test = labels[train_index], labels[test_index]\n",
    "    tf_train_set = tf.data.Dataset.from_tensor_slices((X_train,y_train))\n",
    "    tf_test_set = tf.data.Dataset.from_tensor_slices((X_test,y_test))\n",
    "        \n",
    "    BATCH_SIZE = 32\n",
    "    STEPS_PER_EPOCH = len(X_train)//BATCH_SIZE\n",
    "    tf_train_set = tf_train_set.repeat().batch(BATCH_SIZE)\n",
    "    tf_test_set = tf_test_set.batch(BATCH_SIZE)\n",
    "\n",
    "    # Define the learning rate schedule\n",
    "    lr_schedule = tf.keras.optimizers.schedules.InverseTimeDecay(\n",
    "        0.001,\n",
    "        decay_steps=STEPS_PER_EPOCH * 1000,\n",
    "        decay_rate=1,\n",
    "        staircase=False\n",
    "    )\n",
    "        \n",
    "    def get_optimizer():\n",
    "        return tf.keras.optimizers.Adam(lr_schedule)\n",
    "\n",
    "    optimizer = get_optimizer()\n",
    "        \n",
    "    model = tf.keras.models.Sequential([\n",
    "        tf.keras.Input(shape=(600,)),\n",
    "        tf.keras.layers.Dense(1024, activation='relu',kernel_regularizer=tf.keras.regularizers.l2(0.001)), tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.Dropout(0.5),\n",
    "        tf.keras.layers.Dense(512, activation='relu',kernel_regularizer=tf.keras.regularizers.l2(0.001)),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.Dropout(0.5),\n",
    "        tf.keras.layers.Dense(256, activation='relu',kernel_regularizer=tf.keras.regularizers.l2(0.001)),\n",
    "        tf.keras.layers.Dropout(0.5),\n",
    "        tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "\n",
    "    model.compile(optimizer=optimizer,\n",
    "                loss='binary_crossentropy',\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "    # Implement early stopping\n",
    "    early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "    # Train the model2\n",
    "    model.fit(tf_train_set,\n",
    "                steps_per_epoch=STEPS_PER_EPOCH, \n",
    "                epochs=100,\n",
    "                validation_data = tf_test_set, \n",
    "                callbacks=[early_stopping], \n",
    "                verbose=0)\n",
    "        \n",
    "    predictions = model.predict(X_test)\n",
    "    fpr,tpr, thresholds = roc_curve(y_test,predictions)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    auc_values.append(roc_auc)\n",
    "    print(\"AUROC for test data:\", roc_auc)\n",
    "    all_predictions.append(predictions)\n",
    "    all_test_sets.append(y_test)\n",
    "        \n",
    "    ####Before each iteration ends, save each model###\n",
    "        \n",
    "    save_model= tf.keras.models.save_model(model, os.path.join(os.getcwd(), f'Classification Models/Complete Set 2 Models/ROBOMechDB Complete Set 2 Model Fold {fold}.keras'))\n",
    "\n",
    "    fold += 1\n",
    "\n",
    "fcv_mean_auc = np.mean(auc_values)\n",
    "fcv_std_auc = np.std(auc_values)\n",
    "fcv_min_auc = np.min(auc_values)\n",
    "fcv_max_auc = np.max(auc_values)\n",
    "print(fcv_mean_auc, fcv_min_auc,fcv_max_auc)\n",
    "\n",
    "all_predictions_array = np.round(np.concatenate(all_predictions))\n",
    "all_test_sets_array = np.concatenate(all_test_sets)\n",
    "\n",
    "tp_count = 0\n",
    "fp_count = 0\n",
    "fn_count = 0\n",
    "tn_count = 0\n",
    "\n",
    "for i in range(0,len(all_test_sets_array)):\n",
    "    if(all_test_sets_array[i]==1 and int(all_predictions_array[i]) ==1):\n",
    "        tp_count = tp_count+1\n",
    "\n",
    "for i in range(0,len(all_test_sets_array)):\n",
    "     if(all_test_sets_array[i]==0 and int(all_predictions_array[i])==1):\n",
    "        fp_count =fp_count+1\n",
    "\n",
    "for i in range(0,len(all_test_sets_array)):\n",
    "    if(all_test_sets_array[i]==1 and int(all_predictions_array[i])==0):\n",
    "        fn_count = fn_count+1\n",
    "\n",
    "for i in range(0,len(all_test_sets_array)):\n",
    "    if(all_test_sets_array[i]==0 and int(all_predictions_array[i])==0):\n",
    "        tn_count = tn_count+1\n",
    "\n",
    "tpr = tp_count/(tp_count+fn_count)\n",
    "tnr = tn_count/(tn_count+fp_count)\n",
    "\n",
    "print('tpr:' , tpr)\n",
    "print('tnr:' , tnr)\n",
    "print(fcv_min_auc,fcv_max_auc,fcv_std_auc,fcv_mean_auc)\n",
    "\n",
    "all_predictions_array = np.round(np.concatenate(all_predictions))\n",
    "all_test_sets_array = np.concatenate(all_test_sets)\n",
    "\n",
    "tp_count = 0\n",
    "fp_count = 0\n",
    "fn_count = 0\n",
    "tn_count = 0\n",
    "\n",
    "for i in range(0,len(all_test_sets_array)):\n",
    "    if(all_test_sets_array[i]==1 and int(all_predictions_array[i]) ==1):\n",
    "        tp_count = tp_count+1\n",
    "\n",
    "for i in range(0,len(all_test_sets_array)):\n",
    "     if(all_test_sets_array[i]==0 and int(all_predictions_array[i])==1):\n",
    "        fp_count =fp_count+1\n",
    "\n",
    "for i in range(0,len(all_test_sets_array)):\n",
    "    if(all_test_sets_array[i]==1 and int(all_predictions_array[i])==0):\n",
    "        fn_count = fn_count+1\n",
    "\n",
    "for i in range(0,len(all_test_sets_array)):\n",
    "    if(all_test_sets_array[i]==0 and int(all_predictions_array[i])==0):\n",
    "        tn_count = tn_count+1\n",
    "\n",
    "tpr = tp_count/(tp_count+fn_count)\n",
    "tnr = tn_count/(tn_count+fp_count)\n",
    "\n",
    "print('tpr:' , tpr)\n",
    "print('tnr:' , tnr)\n",
    "print(fcv_min_auc,fcv_max_auc,fcv_std_auc,fcv_mean_auc)\n"
   ],
   "id": "6c0da2ecf1da5617",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "998\n",
      "min 1.3490766481180738\n",
      "mean 3.9296513978117704\n",
      "stdev 1.1837539597778333\n",
      "589\n",
      "min 1.3756730714527872\n",
      "mean 6.979102681442049\n",
      "stdev 2.672071221220308\n",
      "835\n",
      "min 1.00340273053115\n",
      "mean 6.690955269076238\n",
      "stdev 4.105268395143029\n",
      "4.521528377700687\n",
      "8.315138292052202\n",
      "8.743589466647753\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/numpy/core/fromnumeric.py:59: FutureWarning: 'DataFrame.swapaxes' is deprecated and will be removed in a future version. Please use 'DataFrame.transpose' instead.\n",
      "  return bound(*args, **kwds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 1\n",
      "[    1     2     3 ... 17918 17919 17921]\n",
      "\u001B[1m113/113\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 1ms/step  \n",
      "AUROC for test data: 0.7798462260002567\n",
      "Fold: 2\n",
      "[    0     1     2 ... 17919 17920 17921]\n",
      "\u001B[1m113/113\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 1ms/step\n",
      "AUROC for test data: 0.7510099418388104\n",
      "Fold: 3\n",
      "[    0     1     3 ... 17919 17920 17921]\n",
      "\u001B[1m112/112\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 867us/step\n",
      "AUROC for test data: 0.7648760114266913\n",
      "Fold: 4\n",
      "[    0     2     3 ... 17919 17920 17921]\n",
      "\u001B[1m112/112\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 769us/step\n",
      "AUROC for test data: 0.747308938195161\n",
      "Fold: 5\n",
      "[    0     1     2 ... 17916 17918 17920]\n",
      "\u001B[1m112/112\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 785us/step\n",
      "AUROC for test data: 0.7723487596157201\n",
      "0.7630779754153278 0.747308938195161 0.7798462260002567\n",
      "tpr: 0.815726421258985\n",
      "tnr: 0.5675057208237986\n",
      "0.747308938195161 0.7798462260002567 0.012366523926490649 0.7630779754153278\n",
      "tpr: 0.815726421258985\n",
      "tnr: 0.5675057208237986\n",
      "0.747308938195161 0.7798462260002567 0.012366523926490649 0.7630779754153278\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/91/rbzkmg7n5g7f992d83ryjnz40000gn/T/ipykernel_93328/3996162156.py:240: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  if(all_test_sets_array[i]==1 and int(all_predictions_array[i]) ==1):\n",
      "/var/folders/91/rbzkmg7n5g7f992d83ryjnz40000gn/T/ipykernel_93328/3996162156.py:244: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  if(all_test_sets_array[i]==0 and int(all_predictions_array[i])==1):\n",
      "/var/folders/91/rbzkmg7n5g7f992d83ryjnz40000gn/T/ipykernel_93328/3996162156.py:248: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  if(all_test_sets_array[i]==1 and int(all_predictions_array[i])==0):\n",
      "/var/folders/91/rbzkmg7n5g7f992d83ryjnz40000gn/T/ipykernel_93328/3996162156.py:252: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  if(all_test_sets_array[i]==0 and int(all_predictions_array[i])==0):\n",
      "/var/folders/91/rbzkmg7n5g7f992d83ryjnz40000gn/T/ipykernel_93328/3996162156.py:271: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  if(all_test_sets_array[i]==1 and int(all_predictions_array[i]) ==1):\n",
      "/var/folders/91/rbzkmg7n5g7f992d83ryjnz40000gn/T/ipykernel_93328/3996162156.py:275: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  if(all_test_sets_array[i]==0 and int(all_predictions_array[i])==1):\n",
      "/var/folders/91/rbzkmg7n5g7f992d83ryjnz40000gn/T/ipykernel_93328/3996162156.py:279: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  if(all_test_sets_array[i]==1 and int(all_predictions_array[i])==0):\n",
      "/var/folders/91/rbzkmg7n5g7f992d83ryjnz40000gn/T/ipykernel_93328/3996162156.py:283: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  if(all_test_sets_array[i]==0 and int(all_predictions_array[i])==0):\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "###6. #Varying Train:test ratio###\n",
    "\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.metrics import precision_recall_curve, recall_score\n",
    "import numpy as np\n",
    "import itertools\n",
    "\n",
    "stat_array = []\n",
    "train_var = [0.8,0.7,0.6,0.5,0.4,0.3,0.2,0.005]\n",
    "random_states = [42,21,32,10,12]\n",
    "\n",
    "for i in train_var:\n",
    "    print('training:testing:', i*100,(1-i)*100)\n",
    "    auc_values = []\n",
    "    precision_values = []\n",
    "    recall_values = []\n",
    "    for j in range(5):\n",
    "        tf.keras.utils.set_random_seed(j)\n",
    "        shuffled_df = df_2.sample(frac=1.0, random_state=random_states[j])\n",
    "        data = np.array(shuffled_df.iloc[:,3:-1])\n",
    "        labels = np.array(shuffled_df.iloc[:,-1])\n",
    "        shuffled_df = []\n",
    "\n",
    "        train_ratio = i\n",
    "        train_size = int(train_ratio * len(data))\n",
    "        train_data = data[:train_size]\n",
    "        test_data = data[train_size:]\n",
    "        train_labels = labels[:train_size]\n",
    "        test_labels = labels[train_size:]\n",
    "        tf_train_set = tf.data.Dataset.from_tensor_slices((train_data,train_labels))\n",
    "        tf_test_set = tf.data.Dataset.from_tensor_slices((test_data,test_labels))\n",
    "        \n",
    "        BATCH_SIZE = 32\n",
    "        STEPS_PER_EPOCH = len(X_train)//BATCH_SIZE\n",
    "        tf_train_set = tf_train_set.repeat().batch(BATCH_SIZE)\n",
    "        tf_test_set = tf_test_set.batch(BATCH_SIZE)\n",
    "\n",
    "        # Define the learning rate schedule\n",
    "        lr_schedule = tf.keras.optimizers.schedules.InverseTimeDecay(\n",
    "            0.001,\n",
    "            decay_steps=STEPS_PER_EPOCH * 1000,\n",
    "            decay_rate=1,\n",
    "            staircase=False\n",
    "        )\n",
    "        \n",
    "        def get_optimizer():\n",
    "            return tf.keras.optimizers.Adam(lr_schedule)\n",
    "\n",
    "        optimizer = get_optimizer()\n",
    "        \n",
    "        model = tf.keras.models.Sequential([\n",
    "            tf.keras.Input(shape=(600,)),\n",
    "            tf.keras.layers.Dense(1024, activation='relu',kernel_regularizer=tf.keras.regularizers.l2(0.001)), tf.keras.layers.BatchNormalization(),\n",
    "            tf.keras.layers.Dropout(0.5),\n",
    "            tf.keras.layers.Dense(512, activation='relu',kernel_regularizer=tf.keras.regularizers.l2(0.001)),\n",
    "            tf.keras.layers.BatchNormalization(),\n",
    "            tf.keras.layers.Dropout(0.5),\n",
    "            tf.keras.layers.Dense(256, activation='relu',kernel_regularizer=tf.keras.regularizers.l2(0.001)),\n",
    "            tf.keras.layers.Dropout(0.5),\n",
    "            tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "        ])\n",
    "\n",
    "        model.compile(optimizer=optimizer,\n",
    "                loss='binary_crossentropy',\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "        # Implement early stopping\n",
    "        early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "        # Train the model\n",
    "        model.fit(tf_train_set,\n",
    "                steps_per_epoch=STEPS_PER_EPOCH, \n",
    "                epochs=100,\n",
    "                validation_data = tf_test_set, \n",
    "                callbacks=[early_stopping], \n",
    "                verbose=0)\n",
    "        \n",
    "        predictions = model.predict(test_data)\n",
    "        test_predictions_rounded = np.round(predictions)\n",
    "        fpr, tpr, thresholds = roc_curve(test_labels, predictions)\n",
    "        precision,recall,thresholds = precision_recall_curve(test_labels, predictions)\n",
    "        auc_precision_recall = auc(recall, precision)\n",
    "        recall_score_pred = recall_score(test_labels, test_predictions_rounded)\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        auc_values.append(roc_auc)\n",
    "        precision_values.append(auc_precision_recall)\n",
    "        recall_values.append(recall_score_pred)\n",
    "    \n",
    "    print('auc avg:', np.mean(auc_values), 'auc stddev:', np.std(auc_values))\n",
    "    print('auprc avg:', np.mean(precision_values), 'auprc stddev:', np.std(precision_values))\n",
    "    print('recall avg:', np.mean(recall_values), 'recall stddev:', np.std(recall_values))\n",
    "    stat_array.append([np.mean(auc_values),np.mean(precision_values),np.mean(recall_values)])\n",
    "    \n",
    "    #### Exporting AUROC, AUPRC, and Recall values for each train test split as csv\n",
    "import csv\n",
    "statfile = \"stat.csv\"\n",
    "\n",
    "with open(statfile, 'w', newline='') as csvfile:\n",
    "    csvwriter = csv.writer(csvfile)\n",
    "    \n",
    "    # Writing the data row by row\n",
    "    for row in stat_array:\n",
    "        csvwriter.writerow(row)\n"
   ],
   "id": "19d93e7106791d36",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "import csv\n",
    "\n",
    "statfile = \"stat.csv\"\n",
    "\n",
    "with open(statfile, 'w', newline='') as csvfile:\n",
    "    csvwriter = csv.writer(csvfile)\n",
    "\n",
    "    # Writing the data row by row\n",
    "    for row in stat_array:\n",
    "        csvwriter.writerow(row)"
   ],
   "id": "4c5ead9ecd950a69",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "657cfbcf2b3b80a2",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
