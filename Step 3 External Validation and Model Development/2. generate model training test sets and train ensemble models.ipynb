{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### STEP 2. GENERATE INTERNAL VALIDATION SETS & MODEL TRAINING####\n",
    "\n",
    "### RUN ALL THE CODE BLOCK COMBINATIONS BELOW####\n",
    "### MAKE SURE TO RESTART THE KERNEL BEFORE EACH RUN \n",
    "\n",
    "#represents the block number of the code block\n",
    "\n",
    "## Complete Set 1. positive triples + randomNegative triples (without distanceThreshold) —> run #1, #2, #3\n",
    "## Complete Set 2. positive triples + restrictedNegative triples (with distanceThreshold) —> run #1, #2, #4\n"
   ],
   "id": "d4bcc04c9ceaede6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "###FUNCTIONS####\n",
    "\n",
    "def csvtodict(filename):\n",
    "    vector_dict = {}\n",
    "    df_vector_dict = pd.read_csv(filename, header = 0)\n",
    "    keys = df_vector_dict.iloc[:,0].tolist()\n",
    "    values = list(range(len(df_vector_dict)))\n",
    "    for key,value in zip(keys,values):\n",
    "        vector_dict[key] = value\n",
    "    return vector_dict\n",
    "\n"
   ],
   "id": "84388f2685240afe"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-11-13T01:26:22.578761Z",
     "start_time": "2024-11-13T01:26:18.891706Z"
    }
   },
   "source": [
    "#####1.\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "parent_dir = os.path.dirname(os.getcwd())\n",
    "print(parent_dir)\n",
    "\n",
    "df_randNegs = pd.read_csv(os.path.join(parent_dir, 'Step 3 External Validation and Model Development/External Validation Datasets/randomNegatives External Validation Set.csv'))\n",
    "df_restrictedNegs = pd.read_csv(os.path.join(parent_dir, 'Step 3 External Validation and Model Development/External Validation Datasets/restrictedNegatives External Validation Set.csv'))\n",
    "df_diverseNegs = pd.read_csv(os.path.join(parent_dir, 'Step 3 External Validation and Model Development/External Validation Datasets/diverseNegatives External Validation Set.csv'))\n",
    "\n",
    "unified_columns = [f\"{i}\" for i in range(604)]\n",
    "filename = os.path.join(parent_dir, 'Step 1 Data Processing/ROBOKOP+DrugMechDB/ROBOKOP+DrugmechDB Data/ROBOMechDB Processed Triples.csv')\n",
    "df=pd.read_csv(filename)\n",
    "\n",
    "df.columns = ['0','1','2']\n",
    "\n",
    "df_ext = pd.concat([df_randNegs.iloc[:,:3],df_restrictedNegs.iloc[:,:3],df_diverseNegs.iloc[:,:3],df],axis=0)\n",
    "\n",
    "df_80 = df.sample(n=int(0.8*len(df)), random_state=42)  ##take 80% of these positive triples\n",
    "df_80_indices = df_80.index.tolist()\n",
    "\n",
    "pos_neg_triples_dictionary = {}\n",
    "values = my_list = [1] * len(df_ext)\n",
    "keys = [df_ext.iloc[i,0] + \" \" + df_ext.iloc[i,1]+ \" \" + df_ext.iloc[i,2] for i in range(0,len(df_ext))]\n",
    "for key,value in zip(keys,values):\n",
    "    pos_neg_triples_dictionary[key] = value\n",
    "\n",
    "\n",
    "####Find all unique drugs, diseases, and proteins in this 80% set. \n",
    "unique_triples_drug = sorted(list(set(df_80['0'])))\n",
    "unique_triples_disease =sorted(list(set(df_80['1'])))\n",
    "unique_triples_protein = sorted(list(set(df_80['2'])))\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/eding/PycharmProjects/U24-ROBOKOP-Project-8-21-24\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-13T01:26:29.972042Z",
     "start_time": "2024-11-13T01:26:29.849468Z"
    }
   },
   "cell_type": "code",
   "source": [
    "####2 Importing created protein, disease, and drug vector dictionaries into here\n",
    "protein_dict = csvtodict(os.path.join(parent_dir,\n",
    "                                      'Step 2 Data Embedding/Vector Dictionaries/ROBOMechDB Protein Vector Dictionary.csv'))\n",
    "\n",
    "disease_dict = csvtodict(os.path.join(parent_dir,\n",
    "                                      'Step 2 Data Embedding/Vector Dictionaries/ROBOMechDB Disease Vector Dictionary.csv'))\n",
    "\n",
    "drug_dict = csvtodict(os.path.join(parent_dir,\n",
    "                                   'Step 2 Data Embedding/Vector Dictionaries/ROBOMechDB Drug Vector Dictionary.csv'))\n",
    "\n",
    "protein_df = pd.read_csv(os.path.join(parent_dir,\n",
    "                                      'Step 2 Data Embedding/Vector Dictionaries/ROBOMechDB Protein Vector Dictionary.csv'),\n",
    "                         header=0)\n",
    "disease_df = pd.read_csv(os.path.join(parent_dir,\n",
    "                                      'Step 2 Data Embedding/Vector Dictionaries/ROBOMechDB Disease Vector Dictionary.csv'),\n",
    "                         header=0)\n",
    "drug_df = pd.read_csv(os.path.join(parent_dir,\n",
    "                                   'Step 2 Data Embedding/Vector Dictionaries/ROBOMechDB Drug Vector Dictionary.csv'),\n",
    "                      header=0)"
   ],
   "id": "2cedee974867f3c7",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\"\"\"3 Complete Set 1 Development\"\"\"\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import auc, roc_curve\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools\n",
    "import random\n",
    "\n",
    "random_seeds = [1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,42]\n",
    "\n",
    "model_auc_values = []\n",
    "\n",
    "positive_triples_vector_array = []\n",
    "for i in range(0,len(df_80)):\n",
    "    try:\n",
    "        drug_vector = drug_df.iloc[drug_dict[df_80.iloc[j,0]],1:201].tolist()\n",
    "        disease_vector = disease_df.iloc[disease_dict[df_80.iloc[j,1]],1:201].tolist()\n",
    "        protein_vector = protein_df.iloc[protein_dict[df_80.iloc[j,2]],1:201].tolist()\n",
    "        \n",
    "        drug_name = df_80.iloc[j,0]\n",
    "        disease_name = df_80.iloc[j,1]\n",
    "        protein_name = df_80.iloc[j,2]\n",
    "        \n",
    "        row = [[drug_name],[disease_name],[protein_name],drug_vector,disease_vector,protein_vector,[1]]\n",
    "        merged = list(itertools.chain(*row))\n",
    "        positive_triples_vector_array.append(merged)\n",
    "\n",
    "    except KeyError:\n",
    "        continue\n",
    "\n",
    "positive_triples_dataframe = pd.DataFrame(positive_triples_vector_array)\n",
    "\n",
    "for i in random_seeds:\n",
    "    random.seed(i)\n",
    "\n",
    "    negative_triples_array = []\n",
    "    negative_triples_vector_array = []\n",
    "    temp = set()\n",
    "    j=0\n",
    "    while j < int(1.1*len(positive_triples_dataframe)):  ###create negative triples\n",
    "        drug = random.sample(unique_triples_drug,k=1)[0]\n",
    "        protein = random.sample(unique_triples_protein, k=1)[0]\n",
    "        disease = random.sample(unique_triples_disease,k=1)[0]\n",
    "        if (drug + \" \" + disease + \" \" + protein) in pos_neg_triples_dictionary or (drug + \" \" + disease + \" \" + protein) in temp:\n",
    "            continue\n",
    "        temp.add(drug + \" \" + disease + \" \" + protein)\n",
    "        negative_triples_array.append([drug, disease, protein])\n",
    "        j+= 1 \n",
    "\n",
    "    df_negative_triples = pd.DataFrame(negative_triples_array)\n",
    "    j=0\n",
    "    for j in range(0,len(df_negative_triples)):\n",
    "        try:\n",
    "            drug_vector = drug_df.iloc[drug_dict[df_negative_triples.iloc[j,0]],1:201].tolist()\n",
    "            disease_vector = disease_df.iloc[disease_dict[df_negative_triples.iloc[j,1]],1:201].tolist()\n",
    "            protein_vector = protein_df.iloc[protein_dict[df_negative_triples.iloc[j,2]],1:201].tolist()\n",
    "        \n",
    "            drug_name = df_negative_triples.iloc[j,0]\n",
    "            disease_name = df_negative_triples.iloc[j,1]\n",
    "            protein_name = df_negative_triples.iloc[j,2]\n",
    "        \n",
    "            row = [[drug_name],[disease_name],[protein_name],drug_vector,disease_vector,protein_vector,[0]]\n",
    "            merged = list(itertools.chain(*row))\n",
    "            negative_triples_vector_array.append(merged)\n",
    "\n",
    "        except KeyError:\n",
    "            continue\n",
    "\n",
    "    negative_triples_dataframe = pd.DataFrame(negative_triples_vector_array)\n",
    "    df = pd.concat([positive_triples_dataframe,negative_triples_dataframe],axis=0,ignore_index=True)\n",
    "    df.columns = unified_columns\n",
    "    test = pd.concat([df_diverseNegs, df],axis=0)\n",
    "    col = ['0', '1', '2']\n",
    "    print(len(test))\n",
    "    rem = test.drop_duplicates(subset = col)\n",
    "    print(len(rem))\n",
    "    \n",
    "# Assuming df is already defined and contains your dataset\n",
    "    shuffled_df = df.sample(frac=1.0, random_state=i) \n",
    "    data = shuffled_df.iloc[:, 3:-1].values\n",
    "    labels = shuffled_df.iloc[:, -1].values\n",
    "    print(shuffled_df)\n",
    "    num_folds = 5\n",
    "    skf = StratifiedKFold(n_splits=num_folds, shuffle=True, random_state=i)\n",
    "\n",
    "    all_predictions = []\n",
    "    all_test_sets = []\n",
    "\n",
    "    # Perform 5-fold cross-validation to test the accuracy of the model\n",
    "    fold = 1\n",
    "    auc_values = []\n",
    "\n",
    "    tf.keras.utils.set_random_seed(i)\n",
    "\n",
    "    for train_index, test_index in skf.split(data, labels):\n",
    "        print(f\"Fold: {fold}\")\n",
    "    \n",
    "    # Split the data into training and test sets\n",
    "        X_train, X_test = data[train_index], data[test_index]\n",
    "        y_train, y_test = labels[train_index], labels[test_index]\n",
    "        tf_train_set = tf.data.Dataset.from_tensor_slices((X_train,y_train))\n",
    "        tf_test_set = tf.data.Dataset.from_tensor_slices((X_test,y_test))\n",
    "        \n",
    "        BATCH_SIZE = 32\n",
    "        STEPS_PER_EPOCH = len(X_train)//BATCH_SIZE\n",
    "        tf_train_set = tf_train_set.repeat().batch(BATCH_SIZE)\n",
    "        tf_test_set = tf_test_set.batch(BATCH_SIZE)\n",
    "\n",
    "    # Define the learning rate schedule\n",
    "        lr_schedule = tf.keras.optimizers.schedules.InverseTimeDecay(\n",
    "            0.001,\n",
    "            decay_steps=STEPS_PER_EPOCH * 1000,\n",
    "            decay_rate=1,\n",
    "            staircase=False\n",
    "        )\n",
    "        \n",
    "        \n",
    "        def get_optimizer():\n",
    "            return tf.keras.optimizers.Adam(lr_schedule)\n",
    "\n",
    "        optimizer = get_optimizer()\n",
    "        \n",
    "        model = tf.keras.models.Sequential([\n",
    "            tf.keras.Input(shape=(600,)),\n",
    "            tf.keras.layers.Dense(1024, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001)), \n",
    "            tf.keras.layers.Dropout(0.8),\n",
    "            tf.keras.layers.Dense(512, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001)),\n",
    "            tf.keras.layers.Dropout(0.8),\n",
    "            tf.keras.layers.Dense(256, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001)),\n",
    "            tf.keras.layers.Dropout(0.8),\n",
    "            tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "        ])\n",
    "\n",
    "        model.compile(optimizer=optimizer,\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "        # Implement early stopping\n",
    "        early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "        \n",
    "        # Train the model\n",
    "        model.fit(tf_train_set,\n",
    "                    steps_per_epoch=STEPS_PER_EPOCH, \n",
    "                    epochs=40,\n",
    "                    validation_data = tf_test_set, \n",
    "                    callbacks=[early_stopping], \n",
    "                    verbose=0)\n",
    "        \n",
    "        predictions = model.predict(X_test)\n",
    "        fpr,tpr, thresholds = roc_curve(y_test,predictions)\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        auc_values.append(roc_auc)\n",
    "        print(\"AUROC for test data:\", roc_auc)\n",
    "        all_predictions.append(predictions)\n",
    "        all_test_sets.append(y_test)\n",
    "        \n",
    "        ####Before each iteration ends, save each model###\n",
    "        \n",
    "        save_model= tf.keras.models.save_model(model, os.path.join(os.getcwd(), f'Classification Models/Complete Set 1 Models/ROBOMechDB Complete Set 1 Model Seed {i}.keras'))\n",
    "\n",
    "        fold += 1\n",
    "        \n",
    "\n",
    "    fcv_mean_auc = np.mean(auc_values)\n",
    "    fcv_std_auc = np.std(auc_values)\n",
    "    fcv_min_auc = np.min(auc_values)\n",
    "    fcv_max_auc = np.max(auc_values)\n",
    "    print(fcv_mean_auc, fcv_min_auc,fcv_max_auc)\n",
    "    model_auc_values.append([fcv_min_auc,fcv_max_auc,fcv_mean_auc,fcv_std_auc])\n",
    "\n",
    "fcv_df = pd.DataFrame(model_auc_values, columns=['min_auc', 'max_auc', fcv_mean_auc,fcv_std_auc])\n",
    "\n",
    "min_auc = np.min(fcv_df.iloc[:,0])\n",
    "max_auc = np.max(fcv_df.iloc[:,1])\n",
    "avg_auc = np.mean(fcv_df.iloc[:,2])\n",
    "std_dev_auc = np.mean(fcv_df.iloc[:,3])\n",
    "\n",
    "ensemble_stats = np.array([min_auc,max_auc,avg_auc,std_dev_auc])\n",
    "ensemble_stats = ensemble_stats.reshape(1, -1)\n",
    "ensemble_stats_df = pd.DataFrame(ensemble_stats, columns = ['min_auc', 'max_auc', 'avg_auc', 'std_dev_auc'])\n",
    "ensemble_stats_df.to_csv(os.path.join(os.getcwd(), 'Classification Models/Complete Set 1 Models/complete_set_1_ensemble_stats.csv'))\n"
   ],
   "id": "a78e6d57711ce79d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "###4 Complete Set 2 Development\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import auc, roc_curve\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import keras\n",
    "import numpy as np\n",
    "import itertools\n",
    "import random\n",
    "\n",
    "ext_df = pd.read_csv('/Users/eding/Desktop/U24 ROBOKOP Project/External Validation/External Validation Datasets/randomNegatives External Validation Set.csv')\n",
    "\n",
    "model_auc_values = []\n",
    "\n",
    "unique_protein_vectors = np.array(protein_df.iloc[:,1:201])\n",
    "unique_disease_vectors = np.array(disease_df.iloc[:,1:201])\n",
    "unique_drug_vectors = np.array(drug_df.iloc[:,1:201])\n",
    "\n",
    "unique_protein_dict_names = np.array(protein_df.iloc[:,0])\n",
    "unique_disease_dict_names = np.array(disease_df.iloc[:,0])\n",
    "unique_drug_dict_names = np.array(drug_df.iloc[:,0])\n",
    "\n",
    "\n",
    "def compute_threshold(vector_dict,n=0.5):\n",
    "    distances = []\n",
    "    for i in range((len(vector_dict)-1)):\n",
    "        smallest_distance = 10000000\n",
    "        for j in range(i+1, len(vector_dict)):\n",
    "            distance = np.linalg.norm(vector_dict[i]-vector_dict[j])\n",
    "            if distance < smallest_distance:\n",
    "                smallest_distance = distance\n",
    "        distances.append(smallest_distance)\n",
    "\n",
    "    distances = np.array(distances)\n",
    "    print(len(distances))\n",
    "    print('min', np.min(distances))\n",
    "    mean_distances = np.sum(distances)/len(distances)\n",
    "    stdev_distance = np.std(distances)\n",
    "    print('mean',mean_distances)\n",
    "    print('stdev',stdev_distance)\n",
    "\n",
    "    applicability_domain = mean_distances+(n*stdev_distance)\n",
    "\n",
    "    return applicability_domain\n",
    "\n",
    "drug_threshold = compute_threshold(unique_drug_vectors)\n",
    "disease_threshold = compute_threshold(unique_disease_vectors)\n",
    "protein_threshold = compute_threshold(unique_protein_vectors)\n",
    "\n",
    "print(drug_threshold)\n",
    "print(disease_threshold)\n",
    "print(protein_threshold)\n",
    "\n",
    "random_seeds = [1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,42]\n",
    "\n",
    "for i in random_seeds:\n",
    "    random.seed(i)\n",
    "    positive_triples_vector_array = []\n",
    "    for j in range(0,len(df_80)):\n",
    "        try:\n",
    "            drug_vector = drug_df.iloc[drug_dict[df_80.iloc[j,0]],1:201].tolist()\n",
    "            disease_vector = disease_df.iloc[disease_dict[df_80.iloc[j,1]],1:201].tolist()\n",
    "            protein_vector = protein_df.iloc[protein_dict[df_80.iloc[j,2]],1:201].tolist()\n",
    "        \n",
    "            drug_name = df_80.iloc[j,0]\n",
    "            disease_name = df_80.iloc[j,1]\n",
    "            protein_name = df_80.iloc[j,2]\n",
    "        \n",
    "            row = [[drug_name],[disease_name],[protein_name],drug_vector,disease_vector,protein_vector,[1]]\n",
    "            merged = list(itertools.chain(*row))\n",
    "            positive_triples_vector_array.append(merged)\n",
    "\n",
    "        except KeyError:\n",
    "            continue\n",
    "\n",
    "    positive_triples_dataframe = pd.DataFrame(positive_triples_vector_array)\n",
    "\n",
    "    random_rows = positive_triples_dataframe.sample(int(len(positive_triples_dataframe)/4), random_state = i)\n",
    "    \n",
    "\n",
    "    #we are now going to split these rows into 3 equal chunks. \n",
    "    #the triples in the first chunk will have its protein parameter randomized, \n",
    "    #the triples in the second chunk will have its disease parameter randomized, and so on\n",
    "\n",
    "    def generate_negative_triples(data, reference_cols, vector_list, dict_names, threshold, entity_cols, max_count):\n",
    "        negative_triples = []\n",
    "        temp = set()\n",
    "    \n",
    "        for _, row in data.iterrows():\n",
    "            reference = np.array(row[reference_cols[0]:reference_cols[1]]).astype(float)\n",
    "            counter = 0\n",
    "        \n",
    "            for k, vector in enumerate(vector_list):\n",
    "                if np.linalg.norm(reference - vector) < threshold:\n",
    "                    triple_name = [row[col] if col != 'x' else dict_names[k] for col in entity_cols]\n",
    "                    new_combo = \" \".join(triple_name)\n",
    "                \n",
    "                    if new_combo in pos_neg_triples_dictionary or new_combo in temp:\n",
    "                        continue\n",
    "                \n",
    "                    temp.add(new_combo)\n",
    "                    triple_vector = list(itertools.chain(*[np.array(row[3:reference_cols[0]]).tolist(), vector.tolist() if 'x' in entity_cols else np.array(row[reference_cols[1]:603]).tolist()]))\n",
    "                    negative_triples.append(list(itertools.chain(*[triple_name, triple_vector, [0]])))\n",
    "                \n",
    "                    counter += 1\n",
    "                    if counter >= max_count:\n",
    "                        break\n",
    "    \n",
    "        return negative_triples\n",
    "\n",
    "# Split the data\n",
    "    split_rows = np.array_split(random_rows, 3)\n",
    "    drug_disease_x, drug_x_protein, x_disease_protein = split_rows\n",
    "\n",
    "# Generate negative triples for each category\n",
    "    drug_disease_x_negative = generate_negative_triples(drug_disease_x, (403, 603), unique_protein_vectors, unique_protein_dict_names, protein_threshold, [0, 1, 'x'], 6)\n",
    "    drug_x_protein_negative = generate_negative_triples(drug_x_protein, (203, 403), unique_disease_vectors, unique_disease_dict_names, disease_threshold, [0, 'x', 2], 6)\n",
    "    x_disease_protein_negative = generate_negative_triples(x_disease_protein, (3, 203), unique_drug_vectors, unique_drug_dict_names, drug_threshold, ['x', 1, 2], 5)\n",
    "\n",
    "# Combine all negative triples and create the final dataframe\n",
    "    negative_df = pd.concat([pd.DataFrame(triples) for triples in [drug_disease_x_negative,    drug_x_protein_negative, x_disease_protein_negative]], ignore_index=True)\n",
    "    df = pd.concat([positive_triples_dataframe, negative_df], axis=0, ignore_index=True)\n",
    "    df.columns = unified_columns\n",
    "\n",
    "    test = pd.concat([df_diverseNegs, df],axis=0)\n",
    "    col = ['0', '1', '2']\n",
    "    print(len(test))\n",
    "    rem = test.drop_duplicates(subset = col)\n",
    "    print(len(rem))\n",
    "\n",
    "    shuffled_df = df.sample(frac=1.0, random_state=i)\n",
    "\n",
    "    data = shuffled_df.iloc[:,3:-1].values\n",
    "    labels = shuffled_df.iloc[:,-1].values\n",
    "\n",
    "    num_folds = 5\n",
    "    skf = StratifiedKFold(n_splits=num_folds, shuffle=True, random_state=i)\n",
    "\n",
    "    all_predictions = []\n",
    "    all_test_sets = []\n",
    "\n",
    "    # Perform 5-fold cross-validation to test the accuracy of the model######\n",
    "    fold = 1\n",
    "    auc_values = []\n",
    "\n",
    "    keras.utils.set_random_seed(i)\n",
    "    #tf.config.experimental.enable_op_determinism()\n",
    "\n",
    "    shuffled_df = df.sample(frac=1.0, random_state=i)\n",
    "\n",
    "    data = shuffled_df.iloc[:,3:-1].values\n",
    "    labels = shuffled_df.iloc[:,-1].values\n",
    "\n",
    "    num_folds = 5\n",
    "    skf = StratifiedKFold(n_splits=num_folds, shuffle=True, random_state=i)\n",
    "\n",
    "    all_predictions = []\n",
    "    all_test_sets = []\n",
    "\n",
    "    # Perform 5-fold cross-validation to test the accuracy of the model######\n",
    "    fold = 1\n",
    "    auc_values = []\n",
    "\n",
    "    keras.utils.set_random_seed(i)\n",
    "    #tf.config.experimental.enable_op_determinism()\n",
    "\n",
    "    for train_index, test_index in skf.split(data, labels):\n",
    "        print(f\"Fold: {fold}\")\n",
    "        print(train_index)\n",
    "    \n",
    "        # Split the data into training and test sets\n",
    "        X_train, X_test = data[train_index], data[test_index]\n",
    "        y_train, y_test = labels[train_index], labels[test_index]\n",
    "        tf_train_set = tf.data.Dataset.from_tensor_slices((X_train,y_train))\n",
    "        tf_test_set = tf.data.Dataset.from_tensor_slices((X_test,y_test))\n",
    "        \n",
    "        BATCH_SIZE = 32\n",
    "        STEPS_PER_EPOCH = len(X_train)//BATCH_SIZE\n",
    "        tf_train_set = tf_train_set.repeat().batch(BATCH_SIZE)\n",
    "        tf_test_set = tf_test_set.batch(BATCH_SIZE)\n",
    "\n",
    "    # Define the learning rate schedule\n",
    "        lr_schedule = tf.keras.optimizers.schedules.InverseTimeDecay(\n",
    "            0.001,\n",
    "            decay_steps=STEPS_PER_EPOCH * 1000,\n",
    "            decay_rate=1,\n",
    "            staircase=False\n",
    "        )\n",
    "        \n",
    "        def get_optimizer():\n",
    "            return tf.keras.optimizers.Adam(lr_schedule)\n",
    "\n",
    "        optimizer = get_optimizer()\n",
    "        \n",
    "        model = tf.keras.models.Sequential([\n",
    "            tf.keras.Input(shape=(600,)),\n",
    "            tf.keras.layers.Dense(1024, activation='relu',kernel_regularizer=tf.keras.regularizers.l2(0.001)), tf.keras.layers.BatchNormalization(),\n",
    "            tf.keras.layers.Dropout(0.5),\n",
    "            tf.keras.layers.Dense(512, activation='relu',kernel_regularizer=tf.keras.regularizers.l2(0.001)),\n",
    "            tf.keras.layers.BatchNormalization(),\n",
    "            tf.keras.layers.Dropout(0.5),\n",
    "            tf.keras.layers.Dense(256, activation='relu',kernel_regularizer=tf.keras.regularizers.l2(0.001)),\n",
    "            tf.keras.layers.Dropout(0.5),\n",
    "            tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "        ])\n",
    "\n",
    "        model.compile(optimizer=optimizer,\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "        # Implement early stopping\n",
    "        early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "        # Train the model\n",
    "        model.fit(tf_train_set,\n",
    "                        steps_per_epoch=STEPS_PER_EPOCH, \n",
    "                        epochs=100,\n",
    "                        validation_data = tf_test_set, \n",
    "                        callbacks=[early_stopping], \n",
    "                        verbose=0)\n",
    "        \n",
    "        predictions = model.predict(X_test)\n",
    "        fpr,tpr, thresholds = roc_curve(y_test,predictions)\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        auc_values.append(roc_auc)\n",
    "        print(\"AUROC for test data:\", roc_auc)\n",
    "        all_predictions.append(predictions)\n",
    "        all_test_sets.append(y_test)\n",
    "        \n",
    "        ####Before each iteration ends, save each model###\n",
    "        \n",
    "        save_model= tf.keras.models.save_model(model, os.path.join(os.getcwd(), f'Classification Models/Complete Set 2 Models/ROBOMechDB Complete Set 2 Model Seed {i}.keras'))\n",
    "\n",
    "        fold += 1\n",
    "\n",
    "    fcv_mean_auc = np.mean(auc_values)\n",
    "    fcv_std_auc = np.std(auc_values)\n",
    "    fcv_min_auc = np.min(auc_values)\n",
    "    fcv_max_auc = np.max(auc_values)\n",
    "    print(fcv_mean_auc, fcv_min_auc,fcv_max_auc)\n",
    "    model_auc_values.append([fcv_min_auc,fcv_max_auc,fcv_mean_auc,fcv_std_auc])\n",
    "    \n",
    "fcv_df = pd.DataFrame(model_auc_values, columns=['min_auc', 'max_auc', fcv_mean_auc,fcv_std_auc])\n",
    "\n",
    "min_auc = np.min(fcv_df.iloc[:,0])\n",
    "max_auc = np.max(fcv_df.iloc[:,1])\n",
    "avg_auc = np.mean(fcv_df.iloc[:,2])\n",
    "std_dev_auc = np.mean(fcv_df.iloc[:,3])\n",
    "\n",
    "ensemble_stats = np.array([min_auc,max_auc,avg_auc,std_dev_auc])\n",
    "ensemble_stats = ensemble_stats.reshape(1, -1)\n",
    "ensemble_stats_df = pd.DataFrame(ensemble_stats, columns = ['min_auc', 'max_auc', 'avg_auc', 'std_dev_auc'])\n",
    "ensemble_stats_df.to_csv(os.path.join(os.getcwd(), 'Classification Models/Complete Set 2 Models/complete_set_2_ensemble_stats.csv'))"
   ],
   "id": "183944641924dbfe"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
